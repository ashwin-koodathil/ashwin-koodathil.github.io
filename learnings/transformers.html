<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Learning 1: Transformers - Ashwin Koodathil</title>
  <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../style.css" />
</head>
<body>
  <header>
    <div class="header-bg">
      <h1>Learning 1: Transformers</h1>
      <nav>
        <a href="index.html">← Back to Learnings</a>
      </nav>
    </div>
  </header>

  <main>
    <section class="section">
      <h2>Overview</h2>
      <p>
        Transformers are deep learning models introduced in the paper “Attention is All You Need.” They removed recurrence and introduced self-attention, enabling parallelism and efficient handling of long sequences.
      </p>
    </section>

    <section class="section">
      <h3>Key Concepts</h3>
      <ul>
        <li>Self-Attention</li>
        <li>Multi-Head Attention</li>
        <li>Positional Encoding</li>
        <li>Encoder-Decoder Architecture</li>
        <li>Layer Normalization</li>
      </ul>
    </section>

    <section class="section">
      <h3>Applications</h3>
      <p>
        Transformers power models like BERT, GPT, T5, and Vision Transformers (ViT) used in NLP, CV, and even biology (like AlphaFold).
      </p>
    </section>
  </main>

  <footer>
    <p>&copy; 2025 Ashwin Koodathil. All rights reserved.</p>
  </footer>
</body>
</html>
